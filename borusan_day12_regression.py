# -*- coding: utf-8 -*-
"""Borusan_day12_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/160AGNkSDiKh72hCRgr3klRpVPIp0wVNp

Veri kalitesi > Model Karmaşıklığı

Imbalanced Data (Dengesiz Veri)

- SMOTE, class weighting, anomaly detection

Feature Importance & Explainability

- Black box model kurgulamak yeterli değil.

Model Drift Takibi

- Drift Detection teknikleri uygulanmalı ( PSI, KL Divergence )

#Lineer Regresyon

y = f(x) = x0 + a1.x1 + a2.x2 + ..... an.xn

- Penalty ( l1 | l2 ) = l1 : feature selection l2 : katsayı küçültme.  

- Solver

- C ( inverse regularization )
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import Ridge,Lasso
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv("engine_data.csv")

X = df[["Engine rpm","Lub oil pressure","lub oil temp","Fuel pressure","Coolant pressure", "Coolant temp"]]
y = df["Engine Condition"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)

ridge_default = Ridge(alpha=1.0)
ridge_default.fit(X_train, y_train)

lasso_default = Lasso(alpha=1.0, max_iter = 5000)
lasso_default.fit(X_train, y_train)

y_pred_ridge = ridge_default.predict(X_test)
y_pred_lasso = lasso_default.predict(X_test)

print("Ridge Default- MSE:", mean_squared_error(y_test, y_pred_ridge), "R kare skoru:", r2_score(y_test, y_pred_ridge))

!pip install optuna
import optuna

def objective_ridge(trial):
  alpha = trial.suggest_loguniform("alpha", 1e-3, 1e3) #0.001 ile 1000 arasında bir aralık belirlenir.
  model = Ridge(alpha=alpha)
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test) # seçilen alpha ne kadar iyi seçilmiş.
  return -mean_squared_error(y_test, y_pred)

run_ridge = optuna.create_study(direction = "maximize")
run_ridge.optimize(objective_ridge, n_trials = 30)

print("En İyi Parametre:", run_ridge.best_trial.params)

best_ridge = Ridge(alpha=run_ridge.best_params["alpha"])
best_ridge.fit(X_train,y_train)
y_pred_ridge_opt= best_ridge.predict(X_test)

import matplotlib.pyplot as plt
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.scatter(y_test, y_pred_ridge, alpha = 0.5, label="Default Ridge")
plt.scatter(y_test, y_pred_ridge_opt, alpha=0.5, label="Opt Ridge")
plt.plot([y.min(), y.max()], [y.min(),y.max()], "k--")
plt.xlabel("Gerçek Değer")
plt.ylabel("Tahmin")
plt.title("Ridge Regression - Öncesi ve Sonrası")
plt.legend()
plt.show()

"""Veri setinizde çok az parametre, küçük problem varsa (birkaç yüz kombinasyon) GridSearchCV tercih edilebilir.

Veri seti büyükse, aralık genişse ve çok parametre varsa RandomizedSearchCV

Hesap pahalı / verimli denemesi istersen Bayesian Opt( Optuna )

Deep learning / pahalı deneyler, paralel trials Population-Based Trainin veya Optuna + pruner

Multi-objective : bazı durumlarda sadece doğru opt. değil, hız ve hafıza kullanımı da önemli olursa bu hiperparametre opt. tekniğini tercih ederiz.

"""

#GridSearchCV ( Ridge regresyon )

param_grid ={"alpha":[0.01,0.1,1,10,100]}
ridge = Ridge()
grid_search = GridSearchCV(ridge, param_grid, cv = 5, scoring = "neg_mean_squared_error")
grid_search.fit(X_train,y_train)

print("Ridge - GridSearchCV En İyi Parametreler:", grid_search.best_params_)
print("Ridge - En İyi CV Skoru :", -grid_search.best_score_)

algoritmaları yazarken hiperparametrelerinin de aralıklarına değin